{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "#load the handwritten digits dataset\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'images', 'target', 'target_names']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(digits)#returns the information is stored in the object in the form of other objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (type (digits.images)) \n",
    "print (type (digits.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 8, 8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print (digits.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797,)\n",
      "[0 1 2 ... 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "print (digits.target.shape)\n",
    "print (digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 1-dimensional array with 1797 slots. Looking into the array, we see that it contains the true numbers corresponding to each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi(i):\n",
    "    '''Plots 12 digits, starting with digit i'''\n",
    "    nplots = 12\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    for j in range(nplots):\n",
    "        plt.subplot(4,3,j+1)\n",
    "        plt.imshow(digits.images[i+j], cmap='binary')\n",
    "        plt.title(digits.target[i+j])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_multi(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network and preparing the input data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #The input layer requires a 1-dimensional array in input convert 2D-image to 1D-image\n",
    "y = digits.target\n",
    "x = digits.images.reshape((len(digits.images), -1))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first 1000 images and labels are going to be used for training. \n",
    "#The rest of the dataset will be used later to test the performance of our network.\n",
    "x_train = x[:1000]\n",
    "y_train = y[:1000]\n",
    "x_test = x[1000:]\n",
    "y_test = y[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now create the neural network. We use one hidden layers with 15 neurons, \n",
    "#and scikit-learn is smart enough to find out how many numbers to use in the input and output layers.\n",
    "#Don't pay attention to the other parameters\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(15,), activation='logistic', alpha=1e-4,\n",
    "                    solver='sgd', tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.22958289\n",
      "Iteration 2, loss = 1.91207743\n",
      "Iteration 3, loss = 1.62507727\n",
      "Iteration 4, loss = 1.32649842\n",
      "Iteration 5, loss = 1.06100535\n",
      "Iteration 6, loss = 0.83995513\n",
      "Iteration 7, loss = 0.67806075\n",
      "Iteration 8, loss = 0.55175832\n",
      "Iteration 9, loss = 0.45840445\n",
      "Iteration 10, loss = 0.39149735\n",
      "Iteration 11, loss = 0.33676351\n",
      "Iteration 12, loss = 0.29059880\n",
      "Iteration 13, loss = 0.25437208\n",
      "Iteration 14, loss = 0.22838372\n",
      "Iteration 15, loss = 0.20200554\n",
      "Iteration 16, loss = 0.18186565\n",
      "Iteration 17, loss = 0.16461183\n",
      "Iteration 18, loss = 0.14990228\n",
      "Iteration 19, loss = 0.13892154\n",
      "Iteration 20, loss = 0.12833784\n",
      "Iteration 21, loss = 0.12138920\n",
      "Iteration 22, loss = 0.11407971\n",
      "Iteration 23, loss = 0.10677664\n",
      "Iteration 24, loss = 0.10037149\n",
      "Iteration 25, loss = 0.09593187\n",
      "Iteration 26, loss = 0.09250135\n",
      "Iteration 27, loss = 0.08676698\n",
      "Iteration 28, loss = 0.08356043\n",
      "Iteration 29, loss = 0.08209789\n",
      "Iteration 30, loss = 0.07649168\n",
      "Iteration 31, loss = 0.07410898\n",
      "Iteration 32, loss = 0.07126869\n",
      "Iteration 33, loss = 0.06926956\n",
      "Iteration 34, loss = 0.06578496\n",
      "Iteration 35, loss = 0.06374913\n",
      "Iteration 36, loss = 0.06175492\n",
      "Iteration 37, loss = 0.05975664\n",
      "Iteration 38, loss = 0.05764485\n",
      "Iteration 39, loss = 0.05623663\n",
      "Iteration 40, loss = 0.05420966\n",
      "Iteration 41, loss = 0.05413911\n",
      "Iteration 42, loss = 0.05256140\n",
      "Iteration 43, loss = 0.05020265\n",
      "Iteration 44, loss = 0.04902779\n",
      "Iteration 45, loss = 0.04788382\n",
      "Iteration 46, loss = 0.04655532\n",
      "Iteration 47, loss = 0.04586089\n",
      "Iteration 48, loss = 0.04451758\n",
      "Iteration 49, loss = 0.04341598\n",
      "Iteration 50, loss = 0.04238096\n",
      "Iteration 51, loss = 0.04162200\n",
      "Iteration 52, loss = 0.04076839\n",
      "Iteration 53, loss = 0.04003180\n",
      "Iteration 54, loss = 0.03907774\n",
      "Iteration 55, loss = 0.03815565\n",
      "Iteration 56, loss = 0.03791975\n",
      "Iteration 57, loss = 0.03706276\n",
      "Iteration 58, loss = 0.03617874\n",
      "Iteration 59, loss = 0.03593227\n",
      "Iteration 60, loss = 0.03504175\n",
      "Iteration 61, loss = 0.03441259\n",
      "Iteration 62, loss = 0.03397449\n",
      "Iteration 63, loss = 0.03326990\n",
      "Iteration 64, loss = 0.03305025\n",
      "Iteration 65, loss = 0.03244893\n",
      "Iteration 66, loss = 0.03191504\n",
      "Iteration 67, loss = 0.03132169\n",
      "Iteration 68, loss = 0.03079707\n",
      "Iteration 69, loss = 0.03044946\n",
      "Iteration 70, loss = 0.03005546\n",
      "Iteration 71, loss = 0.02960555\n",
      "Iteration 72, loss = 0.02912799\n",
      "Iteration 73, loss = 0.02859103\n",
      "Iteration 74, loss = 0.02825959\n",
      "Iteration 75, loss = 0.02788968\n",
      "Iteration 76, loss = 0.02748725\n",
      "Iteration 77, loss = 0.02721247\n",
      "Iteration 78, loss = 0.02686225\n",
      "Iteration 79, loss = 0.02635636\n",
      "Iteration 80, loss = 0.02607439\n",
      "Iteration 81, loss = 0.02577613\n",
      "Iteration 82, loss = 0.02553642\n",
      "Iteration 83, loss = 0.02518749\n",
      "Iteration 84, loss = 0.02484300\n",
      "Iteration 85, loss = 0.02455379\n",
      "Iteration 86, loss = 0.02432480\n",
      "Iteration 87, loss = 0.02398548\n",
      "Iteration 88, loss = 0.02376004\n",
      "Iteration 89, loss = 0.02341261\n",
      "Iteration 90, loss = 0.02318255\n",
      "Iteration 91, loss = 0.02296065\n",
      "Iteration 92, loss = 0.02274048\n",
      "Iteration 93, loss = 0.02241054\n",
      "Iteration 94, loss = 0.02208181\n",
      "Iteration 95, loss = 0.02190861\n",
      "Iteration 96, loss = 0.02174404\n",
      "Iteration 97, loss = 0.02156939\n",
      "Iteration 98, loss = 0.02119768\n",
      "Iteration 99, loss = 0.02101874\n",
      "Iteration 100, loss = 0.02078230\n",
      "Iteration 101, loss = 0.02061573\n",
      "Iteration 102, loss = 0.02039802\n",
      "Iteration 103, loss = 0.02017245\n",
      "Iteration 104, loss = 0.01997162\n",
      "Iteration 105, loss = 0.01989280\n",
      "Iteration 106, loss = 0.01963828\n",
      "Iteration 107, loss = 0.01941850\n",
      "Iteration 108, loss = 0.01933154\n",
      "Iteration 109, loss = 0.01911473\n",
      "Iteration 110, loss = 0.01905371\n",
      "Iteration 111, loss = 0.01876085\n",
      "Iteration 112, loss = 0.01860656\n",
      "Iteration 113, loss = 0.01848655\n",
      "Iteration 114, loss = 0.01834844\n",
      "Iteration 115, loss = 0.01818981\n",
      "Iteration 116, loss = 0.01798523\n",
      "Iteration 117, loss = 0.01783630\n",
      "Iteration 118, loss = 0.01771441\n",
      "Iteration 119, loss = 0.01749814\n",
      "Iteration 120, loss = 0.01738339\n",
      "Iteration 121, loss = 0.01726549\n",
      "Iteration 122, loss = 0.01709638\n",
      "Iteration 123, loss = 0.01698340\n",
      "Iteration 124, loss = 0.01684606\n",
      "Iteration 125, loss = 0.01667016\n",
      "Iteration 126, loss = 0.01654172\n",
      "Iteration 127, loss = 0.01641832\n",
      "Iteration 128, loss = 0.01630111\n",
      "Iteration 129, loss = 0.01623051\n",
      "Iteration 130, loss = 0.01612736\n",
      "Iteration 131, loss = 0.01590220\n",
      "Iteration 132, loss = 0.01582485\n",
      "Iteration 133, loss = 0.01571372\n",
      "Iteration 134, loss = 0.01560349\n",
      "Iteration 135, loss = 0.01557688\n",
      "Iteration 136, loss = 0.01534420\n",
      "Iteration 137, loss = 0.01527883\n",
      "Iteration 138, loss = 0.01517545\n",
      "Iteration 139, loss = 0.01503663\n",
      "Iteration 140, loss = 0.01501192\n",
      "Iteration 141, loss = 0.01482535\n",
      "Iteration 142, loss = 0.01471388\n",
      "Iteration 143, loss = 0.01463948\n",
      "Iteration 144, loss = 0.01454059\n",
      "Iteration 145, loss = 0.01441742\n",
      "Iteration 146, loss = 0.01431741\n",
      "Iteration 147, loss = 0.01428414\n",
      "Iteration 148, loss = 0.01416364\n",
      "Iteration 149, loss = 0.01406742\n",
      "Iteration 150, loss = 0.01402651\n",
      "Iteration 151, loss = 0.01389720\n",
      "Iteration 152, loss = 0.01381412\n",
      "Iteration 153, loss = 0.01371300\n",
      "Iteration 154, loss = 0.01362465\n",
      "Iteration 155, loss = 0.01357048\n",
      "Iteration 156, loss = 0.01348760\n",
      "Iteration 157, loss = 0.01339543\n",
      "Iteration 158, loss = 0.01331941\n",
      "Iteration 159, loss = 0.01320812\n",
      "Iteration 160, loss = 0.01315415\n",
      "Iteration 161, loss = 0.01308279\n",
      "Iteration 162, loss = 0.01302708\n",
      "Iteration 163, loss = 0.01290042\n",
      "Iteration 164, loss = 0.01289267\n",
      "Iteration 165, loss = 0.01277558\n",
      "Iteration 166, loss = 0.01277238\n",
      "Iteration 167, loss = 0.01261308\n",
      "Iteration 168, loss = 0.01260611\n",
      "Iteration 169, loss = 0.01248789\n",
      "Iteration 170, loss = 0.01239662\n",
      "Iteration 171, loss = 0.01231743\n",
      "Iteration 172, loss = 0.01227346\n",
      "Iteration 173, loss = 0.01223136\n",
      "Iteration 174, loss = 0.01217211\n",
      "Iteration 175, loss = 0.01208682\n",
      "Iteration 176, loss = 0.01204707\n",
      "Iteration 177, loss = 0.01200225\n",
      "Iteration 178, loss = 0.01188677\n",
      "Iteration 179, loss = 0.01184993\n",
      "Iteration 180, loss = 0.01175130\n",
      "Iteration 181, loss = 0.01171178\n",
      "Iteration 182, loss = 0.01166052\n",
      "Iteration 183, loss = 0.01163843\n",
      "Iteration 184, loss = 0.01154892\n",
      "Iteration 185, loss = 0.01147629\n",
      "Iteration 186, loss = 0.01142365\n",
      "Iteration 187, loss = 0.01136608\n",
      "Iteration 188, loss = 0.01128053\n",
      "Iteration 189, loss = 0.01128869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(15,), learning_rate='constant',\n",
       "              learning_rate_init=0.1, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='sgd', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 3, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 5, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply test set\n",
    "predictions = mlp.predict(x_test)\n",
    "predictions[:50] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 0, 5, 3, 6, 9, 6, 1, 7, 5, 4, 4, 7, 2, 8, 2, 2, 5, 7, 9, 5,\n",
       "       4, 4, 9, 0, 8, 9, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4,\n",
       "       5, 6, 7, 8, 9, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:50] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions  close to the targets of our training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9146800501882058"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
